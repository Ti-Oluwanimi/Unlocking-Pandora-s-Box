# Unlocking-Pandora-s-Box
Unlocking Pandora's Box: Unveiling the Elusive Realm of AI Text Detection

## Abstract
<div style="text-align: justify">The proliferation of large language models, such as GPT, has raised concerns about the potential misuse of generated text for academic dishonesty and other negative purposes. In this study, we explore the robustness of AI text detectors in accurately classifying various types of essays, including those generated by GPT. Our experiments reveal the limitations of current text detection systems, highlighting the need for further research and development in this field. We present an evaluation of different text detector tools and discuss their performance on GPT-generated essays. The results underscore the challenges in reliably distinguishing between human-written and GPT-generated content. We also emphasize the importance of refining the models and algorithms employed by text detectors to enhance their robustness and adaptability. Additionally, we advocate for collaboration between large language model companies and text detection organizations to effectively address the multi-sided risks which comes as a result of deploying their tools. By advancing the field of AI text detection, we can ensure the integrity of educational systems and mitigate the risks associated with the misuse of GPT-generated text.</div>

## Main Results
The table below summarizes the classification accuracies of different AI text detectors on the essay samples:

| Essay Type    | Results (%)  | Sapling | Crossplag | OpenAI | ZeroGPT | GPTZero | Con.Scale |
|---------------|--------------|---------|-----------|--------|---------|---------|-----------|
|               | Human (%)    | 50.0    | 100.0     | 100.0  | 83.3    | 83.3    | 100.0     |
| Argumentative | GPT (%)      | 71.4    | 57.1      | 42.9   | 85.7    | 85.7    | 57.1      |
|               | Accuracy (%) | 61.5    | 76.9      | 69.2   | 84.6    | 84.6    | 76.9      |
|               |              |         |           |        |         |         |           |
|               | Human (%)    | 100.0   | 100.0     | 100.0  | 100.0   | 100.0   | 100.0     |
| Descriptive   | GPT (%)      | 44.4    | 55.6      | 11.1   | 55.6    | 66.7    | 37.5      |
|               | Accuracy (%) | 70.6    | 76.5      | 52.9   | 76.5    | 82.4    | 64.7      |
|               |              |         |           |        |         |         |           |
|               | Human (%)    | 100.0   | 100.0     | 80.0   | 80.0    | 80.0    | 100.0     |
| Expository    | GPT (%)      | 50.0    | 50.0      | 33.3   | 66.7    | 50.0    | 33.3      |
|               | Accuracy (%) | 72.7    | 72.7      | 54.5   | 72.7    | 63.6    | 63.6      |
|               |              |         |           |        |         |         |           |
|               | Human (%)    | 80.0    | 100.0     | 100.0  | 100.0   | 80.0    | 100.0     |
| Narrative     | GPT (%)      | 33.3    | 33.3      | 33.3   | 83.3    | 83.3    | 33.3      |
|               | Accuracy (%) | 54.5    | 63.6      | 63.6   | 90.9    | 81.8    | 63.6      |

## Foolproofing the text detectors
<div style="text-align: justify">1. This table demonstrates the robustness of different AI Text Detector tools on GPT-Improved human-written essays. The values represent the detection scores achieved by each tool for different essay types.</div>

| Essay Type    | Sapling | Crossplag | OpenAI | ZeroGPT | GPTZero | Con.Scale |
|---------------|---------|-----------|--------|---------|---------|-----------|
| Argumentative | 0.33    | 0.5       | 0.33   | 0.69    | 0.69    | 0.5       |
| Descriptive   | 0.38    | 0.5       | 0.0    | 0.5     | 0.63    | 0.25      |
| Expository    | 0.4     | 0.4       | 0.16   | 0.48    | 0.32    | 0.2       |
| Narrative     | 0.32    | 0.2       | 0.2    | 0.8     | 0.64    | 0.2       |
